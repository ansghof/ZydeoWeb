Corpus cleanup & pre-processing:
------------------------------------------------------------------------------------
- Filter: where ZH contains lower-case [a-z]
> Source: 6,113,084 segs
  Filtered: 5,653,090 segs

- From ZH, spot segs with traditional; extract; simplify; reconsolidate into BI
> 503,067 segments with trad-only char inside
  Used https://github.com/BYVoid/OpenCC 0.4.3 for conversion

- Create histogram
  ZH, HU char lengths
  Length ratios in each band: Avg; StdDev; Mean
  ZH, HU punct/nonpunct ratios in each band
> lenhists.xlsx

- Filter
  extreme length ratios
  extreme punctuation ratios
  duplicates (lower-cased, w/o punct and spaces, src+trg)
  REMAIN COUNT??
> Dropped: 2,462,433  Kept: 3,181,174

- Tokenize for alignment externally, ZH and HU separately
  For HU, 3 forms: BPE 20k, 30k, 40k

> ZH: https://github.com/fxsjy/jieba
  Installed via pip. Large ditionary downloaded separately.
  python zhotok-jieba.py <..\_work_align\04-tmp-zh.txt >..\_work_align\04-tmp-zh-seg.txt

> HU: export is already lower-cased. Moses tokenizer for raw segmentation b/f BPE
  perl C:\moses-scripts\tokenizer\tokenizer.perl -l hu -no-escape < 04-tmp-hu.txt > 04-tmp-hu-rawseg.txt
  
  https://github.com/rsennrich/subword-nmt
  Actually using modified fork that inserts NMT joiner ￭
  python D:\NMT\Code\subword-nmt\learn_bpe.py --input 04-tmp-hu-rawseg.txt -s 20000 -o hu20k.bpe
  python D:\NMT\Code\subword-nmt\apply_bpe.py -c hu20k.bpe -i 04-tmp-hu.txt -o 04-tmp-hu-bpe20tok.txt --opennmt-separator
  (etc for 30k and 40k)

- Remix tokenized for
  2 extra fields per tokenization: tokenized form; surf range of each token
  these fields exclude punctuation and joiner
  Dedupe along the way
  Also get output for alignment

- Align externally
  https://github.com/clab/fast_align
  Per-word score: https://github.com/gugray/fast_align
  ./fast_align -i 05-tmp-zh-hu20.txt -d -o -v > 05-tmp-zh-hu20.align
  (etc for 30k and 40k)
  20k: cross entropy: 8.15784 perplexity: 285.597
  30k: cross entropy: 8.20877 perplexity: 295.859
  40k: cross entropy: 8.23634 perplexity: 301.567
  Remix into main file
 
- Word freq stats, token length stats, word score histograms, ...


FEED INTO SPHINX
------------------------------------------------------------------------------------
D:\Sphinx\bin >>
indexer -c ../zhhu.conf zhhu
searchd -c ../zhhu.conf

select * from zhhu where match('11');

